---
layout: post
title: Final Report
permalink: /final/
---

# Project Proposal

Click [here](../proposal/) to view our project proposal.

# Progress Report

Click [here](../report/) to view our progress report.

# Analyse
We analyzed the paper "Deep Visual-Semantic Alignments for Generating Image Descriptions" by Andrej Karpathya and Li Fei-Fei and the paper "Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge" by Vinyals et al. from Google.

In Deep Visual-Semantic Alignments for Generating Image Descriptions
# Process
## Introduction

## Implementation

### preprocessing/generator.py

There are two things happen in this file.

The `generator` function reads the data of captions and get a list of all the images. After that, it call `Caption Preprocessor` and feed all the captions so that the `Caption Preprocessor` can build dictionary like structure to store all the words and their index. Then, it call `Image Preprocessor` to convert all the images to an `np.array` with the shape (8000, 299, 299, 3). 8000 is the number of images. 299 is the size of each image. And 3 means three colors (RGB).

### preprocessing/caption_processing.py 

Caption Preprocessor provides functions to convert all the image captions to dictionary like structure. We use a text tokenization utility class called "Tokenizer" from Keras to handle most of the tasks. First, the main 

## Training

# Results

# Conclusion


